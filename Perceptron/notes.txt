Algorithme qui classe un set de données en deux classe (+1, -1)
en utilisant un hyperplan de l'espace séparateur.

Entrée :        S = (xi, yi) 1<=i<=m
                Un # max d'itérations : T
                Un pas d'apprentissage : eta
Initialisation :        w0(t) <- 0
                        w(t) <- o-> de dimension Rd

for t = 1..T
        for i = 1..m
                if( yi(w0(t) + <w(t),xi>) <= 0 )
                        w0(t+1) <- w0(t) + eta.yi
                        w(t+1) <- w(t) + eta.xi.yi
return w0(T+1),


http://archive.ics.uci.edu/ml/

Collections :
        1) Iris
        But : Apprendre à classer les iris setosa des deux autres espéces.
                a) Prendre la base et la transformer de façons à avoir les vecteurs des iris setosa en classe +1 et les autres en  -1
                b) séparer la base en deux de façon aléatoire en prenant 2/3 pour l'apprentissage le reste pour le test. (pour évaluer les resultat -> moyenne d'erreur)

Test :
        perf <- 0
        for i = 1..|BT|
                if yi * (<w(T+1),xi> + w0(T+1)) > 0
                        perf += 1
        return perf/|BT|


Choix du meilleur eta (hyperparamètre)
par validation croisée
        * Diviser S en k ss-ensembles
        * pour eta dans {10⁻3, 10⁻2, 10⁻1, 10⁰,10¹,10²,10³}
                Apprendre K modèles de perceptron sur (K-1) parties de S
                Tester sur la partie restante (pas dans les K-1)
                Faire la moyenne des perfs et prendre le etat ou perf meilleure.


Apprendre le perceptron sur la totalité de S avec le eta ayant la meilleure perf.

Tester l'algo en prennant en compte une partie de S

Entrées :
  Test St = (xi, yi) 1<=i<=N
  wt

perf = 0 **
for i = 1..N
  if yi(<wt, xi>) > 0
    perf += 1

output perf/N (pourcentage)

** >
<(w0t, w1t, w2t, ..., wdt), (1, xi1, xi2, ... xid)>
= w0t + SUM[j=1..d](wjt*xij)

Choix du meilleur hyperparamètre eta € {10e-3, 10e-2, ..., 10e3} avec cross-validation
